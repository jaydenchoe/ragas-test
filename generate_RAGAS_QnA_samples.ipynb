{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaydenchoe/ragas-test/blob/main/generate_RAGAS_QnA_samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate RAGAS synthetic documents**"
      ],
      "metadata": {
        "id": "CRWQ2Q9fZjHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "COLAB = os.getenv(\"COLAB_RELEASE_TAG\") is not None\n",
        "\n",
        "if COLAB:\n",
        "  print ( \"COLAB\" )\n",
        "  !pip install --quiet langchain==0.0.170\n",
        "  !pip install --quiet pyarrow==14.0.1\n",
        "  !pip install --quiet requests==2.31.0\n",
        "  !pip install --quiet cudf-cu12==24.4.1 ibis-framework==8.0.0 google-colab==1.0.0\n",
        "  !pip install --quiet datasets==2.19.0\n",
        "  !pip install --quiet --upgrade langchain-openai\n",
        "  !pip install --quiet pypdf"
      ],
      "metadata": {
        "id": "foxooSxpaqBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f4b6e1-9c94-4fe0-fd7f-7f1b3301d836"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COLAB\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.2/834.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:\n",
        "  print ( \"COLAB\" )\n",
        "  !pip install --quiet \\\n",
        "    chromadb \\\n",
        "    langchain \\\n",
        "    langchain_chroma \\\n",
        "    optuna \\\n",
        "    plotly \\\n",
        "    polars \\\n",
        "    ragas\n",
        "else:\n",
        "  !pip install --quiet \\\n",
        "    chromadb \\\n",
        "    langchain \\\n",
        "    datasets \\\n",
        "    langchain_chroma \\\n",
        "    optuna \\\n",
        "    plotly \\\n",
        "    polars \\\n",
        "    ragas"
      ],
      "metadata": {
        "id": "CznEH6W6ZKwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f203cafa-c683-470e-9db8-c0cbdcbf440c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COLAB\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the packages\n",
        "from functools import reduce\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import warnings\n",
        "\n",
        "import chromadb\n",
        "from chromadb.api.models.Collection import Collection as ChromaCollection\n",
        "from datasets import load_dataset, Dataset\n",
        "from getpass import getpass\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.runnables.base import RunnableSequence\n",
        "from langchain_community.document_loaders import WebBaseLoader, PolarsDataFrameLoader\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from operator import itemgetter\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import polars as pl\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness\n",
        ")\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional"
      ],
      "metadata": {
        "id": "CRN1ubSccUHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c34999-d66d-4215-a83d-3bd6e5df9495"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Providing api key for OPENAI\n",
        "from google.colab import userdata\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab import userdata, data_table\n",
        "  print( \"COLAB\" )\n",
        "  # Secrets\n",
        "  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "  runtime_info = \"Colab runtime\"\n",
        "\n",
        "  # Enabling Colab's data formatter for pandas\n",
        "  data_table.enable_dataframe_formatter()\n",
        "elif OPENAI_API_KEY := os.environ.get('OPENAI_API_KEY'):\n",
        "  # Secrets\n",
        "  runtime_info = \"Non Colab runtime\"\n",
        "else:\n",
        "  OPENAI_API_KEY = getpass(\"OPENAI_API_KEY\")\n",
        "  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "  runtime_info = \"Non Colab runtime\"\n",
        "\n",
        "print(runtime_info)"
      ],
      "metadata": {
        "id": "uVjnwzDtbzjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b99fb69-9f9a-469b-9375-3d71eb08630e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COLAB\n",
            "Colab runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting example docs into vectordb\n",
        "urls = [\"https://en.wikipedia.org/wiki/Large_language_model\"]\n",
        "\n",
        "wikis_loader = WebBaseLoader(urls)\n",
        "wikis = wikis_loader.load()\n",
        "#wikis[0]\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# PDF 파일의 경로를 지정합니다. 실제 경로로 변경해주세요.\n",
        "pdf_path = \"ENN SDK Developer Guide.pdf\"\n",
        "\n",
        "# PyPDFLoader를 사용하여 PDF 파일을 로드합니다.\n",
        "pdf_loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "# PDF 내용을 로드합니다.\n",
        "pdf_pages = pdf_loader.load()\n",
        "\n",
        "# 첫 번째 페이지의 내용을 출력합니다 (선택사항).\n",
        "print(pdf_pages[0].page_content)"
      ],
      "metadata": {
        "id": "I11HlwLrfbt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7e11c1-05a6-48e6-8a20-0818f306ff55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENN SDK Dev eloper Guide\n",
            "Abstract\n",
            "This guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK). It provides instructions for converting Neural Network (NN)\n",
            "models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and\n",
            "obtaining the output.\n",
            "1. Intr oduction\n",
            "ENN SDK  allows users to convert trained TensorFlow Lite  neural network models to a format that can run efficiently in Samsung Exynos  hardware.\n",
            "This guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n",
            "Structur e of Documentation\n",
            "Chapter 1  introduces ENN SDK and its eco-system.\n",
            "Chapter 2  provides information on the features of ENN SDK.\n",
            "Chapter 3  provides information on tools provided with ENN SDK.\n",
            "Chapter 4  provides information on ENN framework API.\n",
            "The subsequent chapters provide additional information on ENN SDK.\n",
            "Samples\n",
            "The list of samples for ENN SDK is available in ENN SDK Samples .\n",
            "Suppor t\n",
            "Support materials including forums, F AQs, and others are available at the Exynos Developer Society web page .\n",
            "Repor ting Bugs\n",
            "To report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues .\n",
            "2. Featur es\n",
            "This chapter provides a general overview of the features that are provided by ENN SDK.\n",
            "Workflow o f ENN SDK\n",
            "Using ENN SDK involves the following two steps:\n",
            "1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n",
            "2. The user executes the converted model for inference.\n",
            "Model Conversion\n",
            "Use one of the tools  that is provided to convert NN models.\n",
            "To convert a model:\n",
            "1. Prepare a pre-trained NN model.\n",
            "2. Set parameters for tools.\n",
            "3. Execute tools for conversion.\n",
            "Model Execution\n",
            "Executing converted models is performed by the ENN framework.\n",
            "When using the ENN framework:\n",
            "1. Initialize ENN framework.\n",
            "2. Load the converted model to ENN framework.\n",
            "3. Allocate and commit all the necessary buffers for the model.\n",
            "Then:\n",
            "1. Copy input data to input buffers.\n",
            "2. Execute model on ENN framework.\n",
            "3. Use data on output buffers.\n",
            "Finally, perform the following steps:To execute the model multiple times, repeat this process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining question evolution types evailable in ragas library\n",
        "llm35 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "llm4 = ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "generator_llm = llm35\n",
        "critic_llm = llm35\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", deployment=\"text-embedding-3-small\")\n",
        "\n",
        "example_generator=None\n",
        "example_generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embeddings,\n",
        "    chunk_size=1024\n",
        ")\n",
        "\n",
        "# Change resulting question type distribution\n",
        "list_of_distributions = [{simple: 1}, {reasoning: 1}, {multi_context: 1}, {conditional: 1}]"
      ],
      "metadata": {
        "id": "fHcL2etvfswm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This step COSTS $$$ ...\n",
        "# Generating the example evolutions\n",
        "#avoid_costs = True\n",
        "avoid_costs = False\n",
        "\n",
        "if not avoid_costs:\n",
        "  # Running ragas to get examples of question evolutions\n",
        "  question_evolution_types = list(map(lambda x: example_generator.generate_with_langchain_docs(pdf_pages, 10, x), list_of_distributions))\n",
        "  print(question_evolution_types)\n",
        "  question_evolution_types_pd = reduce(lambda x, y: pd.concat([x, y], axis=0), [x.to_pandas() for x in question_evolution_types])\n",
        "  print(question_evolution_types_pd)\n",
        "  question_evolution_types_pd = question_evolution_types_pd.loc[:, [\"evolution_type\", \"question\", \"ground_truth\"]]\n",
        "else:\n",
        "  # Downloading examples for question evolutions discussed in the article:\n",
        "  question_evolution_types_pd  = pl.read_csv(\n",
        "    \"https://gist.github.com/gox6/bfd422a6f203ba73f081b08c9bb25e66/raw/example-question-evolution-types-in-ragas.csv\",\n",
        "    separator=\",\",\n",
        ").drop(\"index\").to_pandas()"
      ],
      "metadata": {
        "id": "s75h_9B7f1S4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:\n",
        "  display(data_table.DataTable(question_evolution_types_pd, include_index=False, num_rows_per_page=100))\n",
        "else:\n",
        "  display(question_evolution_types_pd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "GAc0TJV-gAuv",
        "outputId": "f3ddab35-a436-4239-ed1a-e0d44f31fb76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>simple</td>\n",
              "      <td>What are some key models developed by OpenAI i...</td>\n",
              "      <td>GPT-1, GPT-2, GPT-3, GPT-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>reasoning</td>\n",
              "      <td>How do \"sleeper agents\" in LLM models pose sec...</td>\n",
              "      <td>The potential presence of 'sleeper agents' wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>multi_context</td>\n",
              "      <td>How do researchers perceive large language mod...</td>\n",
              "      <td>NLP researchers were split on whether LLMs cou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>conditional</td>\n",
              "      <td>How does toxic content and low-quality data im...</td>\n",
              "      <td>Toxic content and low-quality data impact LLM ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/f565ec32c7e6656f/data_table.js\";\n\n      const table = window.createDataTable({\n        data: [[\"simple\",\n\"What are some key models developed by OpenAI in the field of language processing?\",\n\"GPT-1, GPT-2, GPT-3, GPT-4\"],\n [\"reasoning\",\n\"How do \\\"sleeper agents\\\" in LLM models pose security risks and how can this be addressed?\",\n\"The potential presence of 'sleeper agents' within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.\"],\n [\"multi_context\",\n\"How do researchers perceive large language models in relation to AGI based on their emergent abilities and interpretability through reverse-engineering symbolic algorithms?\",\n\"NLP researchers were split on whether LLMs could understand natural language in a nontrivial sense. Some believe that abilities like mathematical reasoning imply understanding. A Microsoft team suggested that GPT-4 could be seen as an early version of artificial general intelligence. Some researchers view LLMs as 'alien intelligence'. Reverse-engineering symbolic algorithms can help understand LLMs.\"],\n [\"conditional\",\n\"How does toxic content and low-quality data impact LLM training dataset cleaning?\",\n\"Toxic content and low-quality data impact LLM training dataset cleaning by requiring the removal of such passages from the dataset, as well as discarding low-quality data and de-duplicating entries. Cleaning datasets in this manner can enhance training efficiency and improve downstream performance.\"]],\n        columns: [[\"string\", \"evolution_type\"], [\"string\", \"question\"], [\"string\", \"ground_truth\"]],\n        columnOptions: [],\n        rowsPerPage: 100,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n\n      function appendQuickchartButton(parentElement) {\n        let quickchartButtonContainerElement = document.createElement('div');\n        quickchartButtonContainerElement.innerHTML = `\n<div id=\"df-2e25c4ad-bb39-4701-b119-70cd1dd5618f\">\n  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2e25c4ad-bb39-4701-b119-70cd1dd5618f')\"\n            title=\"Suggest charts\"\n            style=\"display:none;\">\n    \n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\">\n    <g>\n        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n    </g>\n</svg>\n  </button>\n  \n<style>\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n</style>\n\n  <script>\n    async function quickchart(key) {\n      const quickchartButtonEl =\n        document.querySelector('#' + key + ' button');\n      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n      quickchartButtonEl.classList.add('colab-df-spinner');\n      try {\n        const charts = await google.colab.kernel.invokeFunction(\n            'suggestCharts', [key], {});\n      } catch (error) {\n        console.error('Error during call to suggestCharts:', error);\n      }\n      quickchartButtonEl.classList.remove('colab-df-spinner');\n      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n    }\n    (() => {\n      let quickchartButtonEl =\n        document.querySelector('#df-2e25c4ad-bb39-4701-b119-70cd1dd5618f button');\n      quickchartButtonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n    })();\n  </script>\n</div>`;\n        parentElement.appendChild(quickchartButtonContainerElement);\n      }\n\n      appendQuickchartButton(table);\n    ",
            "text/plain": [
              "<google.colab.data_table.DataTable object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "rtb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}